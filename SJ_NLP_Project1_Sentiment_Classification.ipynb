{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SJ_NLP_Project1_Sentiment_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNArE3c+yqH+EyLMhKe3gj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SandeepJosyula/NeuralNetworks/blob/master/SJ_NLP_Project1_Sentiment_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRx6OTfN5eCD"
      },
      "source": [
        "# Project 1 - NLP - Sequential Models\n",
        "\n",
        "## Sentiment Classification\n",
        "**Objective**\n",
        "The objective of this project is to build a text classification model that analyses the customer's sentiments\n",
        "based on their reviews in the IMDB database. The model uses a complex deep learning model to build an\n",
        "embedding layer followed by a classification algorithm to analyze the sentiment of the customers.\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "The Dataset of 50,000 movie reviews from IMDB, labelled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocab size of 10,000. As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word\n",
        "\n",
        "**Overview**\n",
        "- Dataset of 50,000 movie reviews from IMDB, labeled by sentiment positive (1) or negative (0)\n",
        "- Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers).\n",
        "- For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
        "- As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
        "\n",
        "Command to import data\n",
        "- `from tensorflow.keras.datasets import imdb`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScspWyLd8I2-"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgDYwEXR8M1s",
        "outputId": "eeaa788f-8975-4b32-fc2e-9037919dab73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h89s6AZ38jha"
      },
      "source": [
        "# Initialize the random number generator\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "# Ignore the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import textwrap"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VAGdSgxCC6S"
      },
      "source": [
        "from keras        import models, regularizers, layers, optimizers, losses, metrics\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, TimeDistributed\n",
        "from keras.utils  import np_utils, to_categorical"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE3Xc5Co6lGO"
      },
      "source": [
        "### Import the data (2 Marks)\n",
        "- Use `imdb.load_data()` method\n",
        "- Get train and test set\n",
        "- Take 10000 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI0tNK-5_b_v"
      },
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZH4sa8BAJlg",
        "outputId": "2d0067ba-8df7-4ad8-c454-62a6fc63e895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "print(\"train_data \", train_data.shape)\n",
        "print(\"train_labels \", train_labels.shape)\n",
        "print(\"~\"*100)\n",
        "print(\"test_data \", test_data.shape)\n",
        "print(\"test_labels \", test_labels.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data  (25000,)\n",
            "train_labels  (25000,)\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "test_data  (25000,)\n",
            "test_labels  (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2Hz6mBJPOEK"
      },
      "source": [
        "### Print shape of features & labels (2 Marks)\n",
        " - Number of reviews\n",
        " - Number of words in each review\n",
        " - Number of labels\n",
        " - Maximum review length\n",
        " - Number of categories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjNuytx2Opbs"
      },
      "source": [
        "**Combine train and test data in to single np array to do the pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYCvJbVzNk1O",
        "outputId": "8067a27a-18b9-4415-8cd4-16b28547604f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "data = np.concatenate((train_data, test_data), axis=0)\n",
        "targets = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "print(\"Number of reviews:\", data.shape)\n",
        "print(\"Label categories:\", np.unique(targets))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "print(\"Maximum count of words in a review: \", max([len(sequence) for sequence in data]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews: (50000,)\n",
            "Label categories: [0 1]\n",
            "Number of unique words: 9998\n",
            "Maximum count of words in a review:  2494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9fb6_FmN5kz",
        "outputId": "793cc487-0eaf-44cc-8ca6-8f577850d2ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "length = [len(i) for i in data]\n",
        "print(\"Maximum review length:\", np.max(length))\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum review length: 2494\n",
            "Average Review length: 234.75892\n",
            "Standard Deviation: 173.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUoiEt-ON_mK",
        "outputId": "33f6a4e6-4204-48b4-859e-bf19f1411154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#print the label\n",
        "print(\"Label:\", targets[100])\n",
        "# print the data \n",
        "print(data[100])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 0\n",
            "[1, 13, 244, 6, 87, 337, 7, 628, 2219, 5, 28, 285, 15, 240, 93, 23, 288, 549, 18, 1455, 673, 4, 241, 534, 3635, 8448, 20, 38, 54, 13, 258, 46, 44, 14, 13, 1241, 7258, 12, 5, 5, 51, 9, 14, 45, 6, 762, 7, 2, 1309, 328, 5, 428, 2473, 15, 26, 1292, 5, 3939, 6728, 5, 1960, 279, 13, 92, 124, 803, 52, 21, 279, 14, 9, 43, 6, 762, 7, 595, 15, 16, 2, 23, 4, 1071, 467, 4, 403, 7, 628, 2219, 8, 97, 6, 171, 3596, 99, 387, 72, 97, 12, 788, 15, 13, 161, 459, 44, 4, 3939, 1101, 173, 21, 69, 8, 401, 2, 4, 481, 88, 61, 4731, 238, 28, 32, 11, 32, 14, 9, 6, 545, 1332, 766, 5, 203, 73, 28, 43, 77, 317, 11, 4, 2, 953, 270, 17, 6, 3616, 13, 545, 386, 25, 92, 1142, 129, 278, 23, 14, 241, 46, 7, 158]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M39zwSBFK9K"
      },
      "source": [
        "### Decode the feature value to get original sentence (2 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkrDyJqsFaIt"
      },
      "source": [
        "First, retrieve a dictionary that contains mapping of words to their index in the IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9co9F7HAJdn"
      },
      "source": [
        "# See an actual review in words\n",
        "# Reverse from integers to words using the DICTIONARY (given by keras...need to do nothing to create it)\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "reverse_word_index = dict(\n",
        "[(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9CM7UDFFdlh"
      },
      "source": [
        "Now use the dictionary to get the original words from the encodings, for a particular sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRAll1kUC4T3",
        "outputId": "ff1b4f3e-ec24-450d-dfdd-7da0a74b1c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "pick_idx = 123  # Pick 123rd sentence\n",
        "\n",
        "print(\"Label:\", train_labels[pick_idx])\n",
        "\n",
        "# build the sentence by decoding each word and concatenate it\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '$') for i in train_data[pick_idx]])\n",
        "\n",
        "print(textwrap.fill(decoded_review,100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n",
            "$ beautiful and touching movie rich colors great settings good acting and one of the most charming\n",
            "movies i have seen in a while i never saw such an interesting setting when i was in china my wife\n",
            "liked it so much she asked me to $ on and rate it so other would enjoy too\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXQy_10nTjTr"
      },
      "source": [
        "### Print value of any one feature and it's label (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqzRUmqKTlKj",
        "outputId": "edb1238d-da8f-4bb8-ea27-8502e3bc4e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "pick_idx = 1500 # Pick 1500th review\n",
        "print(\"Label:\", train_labels[pick_idx])\n",
        "# build the sentence by decoding each word and concatenate it\n",
        "decoded_review = ' '.join([reverse_word_index.get(i - 3, '$') for i in train_data[pick_idx]])\n",
        "\n",
        "print(textwrap.fill(decoded_review,100))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n",
            "$ one of the things about the film that $ my heart strings was that dry fly fishing was a major part\n",
            "of the scene i have occasionally carried out my times of dry fly fishing having tied my own flies\n",
            "and being accompanied by my brother and my father we spend a day on one river or another seeking to\n",
            "$ the ever $ brown $ to rise and take the fly that has been offered to them br br when we had\n",
            "occasions like this any differences between us disappeared and any of the $ of the world $ away to\n",
            "be replaced by the glory of being absorbed in the activity and the surroundings of the place we were\n",
            "in br br this was one of the amazing things that was portrayed to me in the film as the minister and\n",
            "his two sons norman and $ carried out the ritual for there is something $ about fly fishing as there\n",
            "is something $ about so many $ you can't just start casting your fishing line and hope for the best\n",
            "you have to $ yourself to the place you are in you have to $ the surface of the water considering\n",
            "how it is flowing and where the best point might be to place your fly and depending on your skill\n",
            "level you might even get your fly to land there long enough for a fish to take note of it and strike\n",
            "the $ of fly $ was directed and represented so well that they themselves can be classified as\n",
            "artists br br the title for the film could not be more $ chosen for the river did in fact run\n",
            "through the life of father and two sons this film however $ itself $ than the family and community\n",
            "in montana by the the $ river where the film is played out it has the capacity to draw you in to $\n",
            "you to capture you as the history of the family community and period is $ the story told is not just\n",
            "a family history but a history of life what may be classified as a $ of $\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5VqjSwUS4S4"
      },
      "source": [
        "### Pad each sentence to be of same length (2 Marks)\n",
        "- Take maximum sequence length as 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh7YsmdQYkwp",
        "outputId": "3f9ba35c-e1ba-4139-dea8-78d5d189c041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X = pad_sequences(data,maxlen=300)\n",
        "print(\"Maximum count of words in a review in original review dataset: \", max([len(sequence) for sequence in data]))\n",
        "print(\"Maximum count of words in a review in padded dataset: \", max([len(sequence) for sequence in X]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum count of words in a review in original review dataset:  2494\n",
            "Maximum count of words in a review in padded dataset:  300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liktIdFZcC0y"
      },
      "source": [
        "### Split in to train and test sets\n",
        " - Train set containing 40000 reviews\n",
        " - Test set containing 10000 reviews\n",
        " - Within the Train set, 10000 reviews are moved in to validation set, and rest 30000 for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou1_GLcKcB1o"
      },
      "source": [
        "x_train = X[10000:]\n",
        "x_test = X[:10000]\n",
        "y_train = targets[10000:]\n",
        "y_test = targets[:10000]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LkUVcFgBkft",
        "outputId": "73d56c7c-552f-4dbc-85f2-40b85fe00457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Set a VALIDATION set\n",
        "\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]\n",
        "\n",
        "print(\"x_val \", x_val.shape)\n",
        "print(\"partial_x_train \", partial_x_train.shape)\n",
        "print(\"y_val \", y_val.shape)\n",
        "print(\"partial_y_train \", partial_y_train.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_val  (10000, 300)\n",
            "partial_x_train  (30000, 300)\n",
            "y_val  (10000,)\n",
            "partial_y_train  (30000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC6HtTzzXFn3"
      },
      "source": [
        "### Define model (10 Marks)\n",
        "- Define a Sequential Model\n",
        "- Add Embedding layer\n",
        "  - Embedding layer turns positive integers into dense vectors of fixed size\n",
        "  - `tensorflow.keras` embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unique integer number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn LabelEncoder.\n",
        "  - Size of the vocabulary will be 10000\n",
        "  - Give dimension of the dense embedding as 100\n",
        "  - Length of input sequences should be 300\n",
        "- Add LSTM layer\n",
        "  - Pass value in `return_sequences` as True\n",
        "- Add a `TimeDistributed` layer with 100 Dense neurons\n",
        "- Add Flatten layer\n",
        "- Add Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73mjykywBkc5",
        "outputId": "2a5417b2-4395-48a9-9d95-4479536d5907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# NN MODEL\n",
        "\n",
        "vocabsize = 10000\n",
        "embed_dim = 100\n",
        "lstm_out = 100\n",
        "\n",
        "# Use of DROPOUT\n",
        "model = models.Sequential()\n",
        "model.add(Embedding(vocabsize, embed_dim,input_length = X.shape[1],name=\"EmbeddingLayer\"))\n",
        "model.add(SpatialDropout1D(0.4,name=\"SpatialDropoutLayer\"))\n",
        "model.add(LSTM(lstm_out,dropout=0.2,recurrent_dropout=0.2,return_sequences=True,name=\"LSTMLayer\"))\n",
        "model.add(TimeDistributed(Dense(100),name=\"TimeDistributedLayer\"))\n",
        "model.add(Flatten(name=\"FlattenLayer\"))\n",
        "model.add(Dense(2,activation='softmax',name=\"FinalDenseLayer\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer LSTMLayer will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lHQwizhow7P"
      },
      "source": [
        "### Compile the model (2 Marks)\n",
        "- Use Optimizer as Adam\n",
        "- Use Binary Crossentropy as loss\n",
        "- Use Accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EctVvETko3q8"
      },
      "source": [
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKy05jUwpENK"
      },
      "source": [
        "### Print model summary (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1dKnT75pB8f",
        "outputId": "08b1f092-08f8-4ae5-a505-0c48480399e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "EmbeddingLayer (Embedding)   (None, 300, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "SpatialDropoutLayer (Spatial (None, 300, 100)          0         \n",
            "_________________________________________________________________\n",
            "LSTMLayer (LSTM)             (None, 300, 100)          80400     \n",
            "_________________________________________________________________\n",
            "TimeDistributedLayer (TimeDi (None, 300, 100)          10100     \n",
            "_________________________________________________________________\n",
            "FlattenLayer (Flatten)       (None, 30000)             0         \n",
            "_________________________________________________________________\n",
            "FinalDenseLayer (Dense)      (None, 2)                 60002     \n",
            "=================================================================\n",
            "Total params: 1,150,502\n",
            "Trainable params: 1,150,502\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r78VaSgjpaM1"
      },
      "source": [
        "### Fit the model (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jz9438gBkaA",
        "outputId": "c3d1a241-c703-4fa3-8311-1004a9c506b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        }
      },
      "source": [
        "# FIT / TRAIN model\n",
        "\n",
        "NumEpochs = 10\n",
        "BatchSize = 5000\n",
        "#model.fit(partial_x_train, partial_y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(x_val, y_val))\n",
        "\n",
        "history = model.fit(partial_x_train, partial_y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(x_val, y_val))\n",
        "\n",
        "results = model.evaluate(x_test, y_test)\n",
        "print(\"_\"*100)\n",
        "print(\"Test Loss and Accuracy\")\n",
        "print(\"results \", results)\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-88a28fbda8a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model.fit(partial_x_train, partial_y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(x_val, y_val))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNumEpochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[5000,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node zeros_like (defined at <ipython-input-18-88a28fbda8a9>:7) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/sequential/EmbeddingLayer/embedding_lookup/Reshape/_26]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[5000,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node zeros_like (defined at <ipython-input-18-88a28fbda8a9>:7) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_3434]\n\nFunction call stack:\ntrain_function -> train_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zmFlbDrB5ZZ"
      },
      "source": [
        "# VALIDATION LOSS curves\n",
        "\n",
        "plt.clf()\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, (len(history_dict['loss']) + 1))\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giOMX3hmB5WI"
      },
      "source": [
        "# VALIDATION ACCURACY curves\n",
        "\n",
        "plt.clf()\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, (len(history_dict['accuracy']) + 1))\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-e3cj5V7Xrl"
      },
      "source": [
        "### Evaluate model (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO7mKG7j7ozi"
      },
      "source": [
        "#### Add your code here ####\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmX-3V3N7XiY"
      },
      "source": [
        "### Predict on one sample (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euMv5gznBP36"
      },
      "source": [
        "def preprocess(sample_text):\n",
        "  sample_input = sample_text.split(' ')\n",
        "  sample_predict = np.zeros(300)\n",
        "  sample_predict = sample_predict.astype('int')\n",
        "  idx = 0\n",
        "  for txt in sample_input:\n",
        "    tmp = word_index.get(txt,0)\n",
        "    sample_predict[idx] = tmp\n",
        "    print(txt,\" - \", tmp)\n",
        "    idx = idx + 1\n",
        "  print(\"Sample Text encoded: \", sample_predict)\n",
        "  sample_predict = sample_predict.astype('int')\n",
        "\n",
        "  # build the final array for predicting using the model\n",
        "  final_pred[0]=sample_predict    #copy the new values to first row\n",
        "  print(\"Final input to model to predict, shape:\", final_pred.shape)\n",
        "  return final_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkjNnTT79tCv"
      },
      "source": [
        "sample_text = \"This is a wonderful movie and I liked it\"\n",
        "final_pred = x_test[:1]         #copy the same structure as test dataset\n",
        "final_pred = preprocess(sample_text)\n",
        "sentiment = model.predict(final_pred,batch_size=1,verbose = 2)[0]\n",
        "if(np.argmax(sentiment) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "    print(\"positive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wDGcEQ2Qpqb"
      },
      "source": [
        "sample_text = \"good movie\"\n",
        "final_pred = x_test[:1]         #copy the same structure as test dataset\n",
        "final_pred = preprocess(sample_text)\n",
        "sentiment = model.predict(final_pred,batch_size=1)[0]\n",
        "if(np.argmax(sentiment) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "    print(\"positive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVA_zoaYR-ou"
      },
      "source": [
        "x = model.predict(final_pred,batch_size=1)[0]\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}